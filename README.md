Knowledge and understanding of
The transformer architecture and attention mechanism.
LLM pre-training of foundation models.
Developments in the evolution of LLMs.
Empirical scaling laws across data size, compute budget and inference requirements.
LLM optimization techniques such as quantization, distillation and pruning.
LLMs like LLaMA, Gemma, Mistral, GPT, DeepSeek, and BERT.
Synthetic LLM data generation.
Selective, reparameterized and additive parameter efficient fine-tuning (PEFT) techniques.
Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) for reinforcement learning with human feedback (RLHF).
RAG, including the LangChain Data Ecosystem, vector databases and external applications.

Skills to
Apply key Hugging Face libraries for loading datasets, building LLM pipelines and model evaluation.
Adapt LLMs using one- or few-shot prompt engineering.
Control model output using generative configuration parameters.
Adapt LLMs to local data or a desired style using Quantized Low-Rank Adaption of Large Language Models (QLoRA).
Align LLMs to human values using RLHF.
Evaluate LLMs using metrics such as ROUGE and BLEU and benchmarks such as HELM.
Augment LLMs to have access to external data sources or applications using RAG.

Competences to
Use LLMs for common NLP tasks such as summarization, chat and question-answering.
Identify which LLM and which adaption technique is best suited for a given problem.
Maximize the performance of LLMs within the specific constraints of a project.
Build an end-to-end LLM-powered application.
