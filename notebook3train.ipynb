{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fed14e79ddf4a2a927bb50a18344cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f4971b8eb7b4764a6b5dd0e9a971071",
              "IPY_MODEL_7ec1e86d6e1e4b7ea4a33b4b99909039",
              "IPY_MODEL_1ced22a73ebb4aaebd361953fe0564e3"
            ],
            "layout": "IPY_MODEL_36be749f48f140fabd43960df22a003e"
          }
        },
        "3f4971b8eb7b4764a6b5dd0e9a971071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b59f402e14d14c9882cfed694259230f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b3d3087e86874e15ad72d83a8a38402c",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "7ec1e86d6e1e4b7ea4a33b4b99909039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6335b3f8fa4145d681a86ce3afe01757",
            "max": 2275264008,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0deb315e620f4858815c5495a31896de",
            "value": 2275264008
          }
        },
        "1ced22a73ebb4aaebd361953fe0564e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4d31902ee1649aba2ec7c535aa66762",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5917bd3e2449443583eeae125634c857",
            "value": "‚Äá2.28G/2.28G‚Äá[00:37&lt;00:00,‚Äá92.3MB/s]"
          }
        },
        "36be749f48f140fabd43960df22a003e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b59f402e14d14c9882cfed694259230f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3d3087e86874e15ad72d83a8a38402c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6335b3f8fa4145d681a86ce3afe01757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0deb315e620f4858815c5495a31896de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4d31902ee1649aba2ec7c535aa66762": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5917bd3e2449443583eeae125634c857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "DdAZtDFs_QjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d93d45-c986-42cd-c5fc-55e99fbc27a3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:41:41.591883Z",
          "iopub.execute_input": "2025-09-27T22:41:41.592163Z",
          "iopub.status.idle": "2025-09-27T22:41:54.135893Z",
          "shell.execute_reply.started": "2025-09-27T22:41:41.592138Z",
          "shell.execute_reply": "2025-09-27T22:41:54.134937Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 0. Config ----------\n",
        "MAX_EXAMPLES = 1000 # change to None to run full test set (may be large)\n",
        "DEVICE = 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\n",
        "SMOL_MODEL = 'HuggingFaceTB/SmolLM-135M-Instruct'\n",
        "PEGASUS_MODEL = 'google/pegasus-cnn_dailymail'\n",
        "\n",
        "MAX_TOKENS = 90\n",
        "MIN_LENGTH = 30"
      ],
      "metadata": {
        "id": "ajhbUMhiULik",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:41:54.137645Z",
          "iopub.execute_input": "2025-09-27T22:41:54.138304Z",
          "iopub.status.idle": "2025-09-27T22:42:01.393626Z",
          "shell.execute_reply.started": "2025-09-27T22:41:54.138277Z",
          "shell.execute_reply": "2025-09-27T22:42:01.393007Z"
        }
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 1. Imports ----------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import os,json"
      ],
      "metadata": {
        "id": "3OgKLHjH_TOY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:42:01.394297Z",
          "iopub.execute_input": "2025-09-27T22:42:01.394591Z",
          "iopub.status.idle": "2025-09-27T22:42:34.329854Z",
          "shell.execute_reply.started": "2025-09-27T22:42:01.394574Z",
          "shell.execute_reply": "2025-09-27T22:42:34.329296Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 2. Helpers: load data ----------\n",
        "\n",
        "\n",
        "def load_cnn_dailymail(split='test', max_examples=None):\n",
        "  ds = load_dataset('cnn_dailymail', '3.0.0', split=split)\n",
        "  if max_examples is not None:\n",
        "    ds = ds.select(range(max_examples))\n",
        "  return ds"
      ],
      "metadata": {
        "id": "I8rlxz1y_U8n",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:42:34.331563Z",
          "iopub.execute_input": "2025-09-27T22:42:34.332097Z",
          "iopub.status.idle": "2025-09-27T22:42:34.336105Z",
          "shell.execute_reply.started": "2025-09-27T22:42:34.332077Z",
          "shell.execute_reply": "2025-09-27T22:42:34.335559Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 3. Model loaders ----------\n",
        "def load_smol(model_name=SMOL_MODEL):\n",
        "  tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True,chat_template=None)\n",
        "  tok.padding_side = 'left'  # Left padding to avoid decoder-only warnings\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True).to(DEVICE)\n",
        "# ensure pad token\n",
        "  if tok.pad_token is None:\n",
        "    tok.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tok))\n",
        "  return tok, model\n",
        "\n",
        "\n",
        "def load_pegasus(model_name=PEGASUS_MODEL):\n",
        "  tok = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(DEVICE)\n",
        "  return tok, model"
      ],
      "metadata": {
        "id": "gLIUXa4M_XCr",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:42:34.336695Z",
          "iopub.execute_input": "2025-09-27T22:42:34.336879Z",
          "iopub.status.idle": "2025-09-27T22:42:34.363750Z",
          "shell.execute_reply.started": "2025-09-27T22:42:34.336864Z",
          "shell.execute_reply": "2025-09-27T22:42:34.363074Z"
        }
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 4. Prompt definitions (Unified for 1-2 Sentence Summary) ----------\n",
        "\n",
        "def truncate_article(text, tokenizer, max_tokens=512):\n",
        "    \"\"\"Truncate article to ~512 tokens to prevent overflow when few-shot examples are added.\"\"\"\n",
        "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
        "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "# instruction prompt\n",
        "def prompt_instruction(article, tokenizer=None):\n",
        "    if tokenizer:\n",
        "        article = truncate_article(article, tokenizer)\n",
        "    return (\n",
        "        \"You are an advanced news summarization assistant.\\n\"\n",
        "        \"Write a detailed, factual summary of the following article in 3‚Äì4 complete sentences.\\n\"\n",
        "        \"Focus on the key events, people, and outcomes. Avoid repetition or speculation.\\n\\n\"\n",
        "        f\"Article:\\n{article.strip()}\\n\\n\"\n",
        "        \"Summary:\"\n",
        "    )\n",
        "\n",
        "FEW_SHOT_EXAMPLES = [\n",
        "    (\n",
        "        # Á§∫‰æãÔºöÁØáÂπÖÊõ¥ÈïøÁöÑÊñ∞ÈóªÔºà‚âà10Âè•Ôºâ\n",
        "        \"A powerful winter storm has blanketed much of the northeastern United States in heavy snow, disrupting travel and closing schools across several states. \"\n",
        "        \"New York City received more than eight inches of snow overnight, while parts of Massachusetts and Connecticut reported over a foot. \"\n",
        "        \"Thousands of flights were canceled or delayed, leaving travelers stranded at major airports. \"\n",
        "        \"Commuters faced treacherous road conditions, and authorities urged residents to stay home unless absolutely necessary. \"\n",
        "        \"In Boston, city officials declared a snow emergency and deployed hundreds of plows to clear main roads. \"\n",
        "        \"Utility companies reported widespread power outages as strong winds knocked down trees and power lines. \"\n",
        "        \"Amtrak suspended most regional train services until conditions improved. \"\n",
        "        \"Meteorologists say the storm is expected to weaken by Thursday afternoon but warned of another system developing early next week. \"\n",
        "        \"Despite the disruption, some residents took advantage of the snowfall to go sledding and build snowmen in city parks. \"\n",
        "        \"Officials reminded the public to check on elderly neighbors and keep emergency supplies at home in case of extended outages.\",\n",
        "\n",
        "        # ‰∏âÂà∞ÂõõÂè•ÊÄªÁªìÔºàÂØπÂ∫îËæìÂá∫Ôºâ\n",
        "        \"Summary: A major winter storm brought heavy snow and high winds to the U.S. Northeast, disrupting travel and shutting down schools. \"\n",
        "        \"More than a foot of snow fell in some areas, causing widespread flight cancellations and power outages. \"\n",
        "        \"Officials declared emergencies, urged residents to stay indoors, and warned of another storm approaching next week. \"\n",
        "        \"The system is expected to weaken by Thursday afternoon as cleanup efforts continue.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "def prompt_few_shot(article, tokenizer=None):\n",
        "    if tokenizer:\n",
        "        article = truncate_article(article, tokenizer)\n",
        "    # Áî®Âçï‰∏™‰ª£Ë°®ÊÄßÁ§∫‰æã\n",
        "    example_article, example_summary = FEW_SHOT_EXAMPLES[0]\n",
        "    return (\n",
        "        \"You are an advanced summarization assistant.\\n\"\n",
        "        \"Here are examples of how to summarize news articles in 3‚Äì4 sentences:\\n\\n\"\n",
        "        f\"Example article:\\n{example_article}\\n{example_summary}\\n\\n\"\n",
        "        f\"Now summarize the following article in 3‚Äì4 sentences:\\n{article}\\n\\nSummary:\"\n",
        "    )"
      ],
      "metadata": {
        "id": "aBgV855h_ZEb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:42:34.364688Z",
          "iopub.execute_input": "2025-09-27T22:42:34.365339Z",
          "iopub.status.idle": "2025-09-27T22:42:34.384441Z",
          "shell.execute_reply.started": "2025-09-27T22:42:34.365307Z",
          "shell.execute_reply": "2025-09-27T22:42:34.383783Z"
        }
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 5. Generation wrappers ----------\n",
        "\n",
        "\n",
        "# best-effort function to strip prompt prefix from generated token sequence\n",
        "def strip_prefix(prompt, text):\n",
        "  p = prompt.strip()\n",
        "  t = text.strip()\n",
        "  if not p:\n",
        "    return t\n",
        "  # if model echoes prompt, remove the longest common prefix\n",
        "  if t.startswith(p):\n",
        "    return t[len(p):].strip()\n",
        "  # also try removing up to first newline after prompt\n",
        "  return t\n",
        "\n",
        "def generate_smol(tokenizer, model, prompts, strategy, max_new_tokens=90, min_length=50, batch_size=8):\n",
        "    \"\"\"\n",
        "    Safe version for decoder-only models.\n",
        "    Ensures one output per input prompt.\n",
        "    Supports Beam, Top-p, Contrastive search, num_return_sequences>1.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch = prompts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=1024).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                min_length=min_length,\n",
        "                **strategy\n",
        "            )\n",
        "\n",
        "        texts = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "        # Handle num_return_sequences > 1\n",
        "        num_return = strategy.get('num_return_sequences', 1)\n",
        "        if num_return > 1:\n",
        "          grouped = [texts[j*num_return:(j+1)*num_return] for j in range(len(batch))]\n",
        "          texts = [g[0] for g in grouped]   # ÊØè‰∏™Ê†∑Êú¨Âè™‰øùÁïôÁ¨¨‰∏Ä‰∏™ÂÄôÈÄâ\n",
        "\n",
        "\n",
        "        # Ensure outputs match batch length\n",
        "        assert len(texts) == len(batch), f\"Mismatch in decoded outputs: {len(texts)} vs batch {len(batch)}\"\n",
        "\n",
        "        outputs.extend([strip_prefix(p, t) for p, t in zip(batch, texts)])\n",
        "\n",
        "    # Ensure total outputs match total prompts\n",
        "    assert len(outputs) == len(prompts), f\"Total outputs {len(outputs)} != total prompts {len(prompts)}\"\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "def generate_pegasus(tokenizer, model, articles, max_new_tokens=90, min_length=50, strategy=None, batch_size=8):\n",
        "    \"\"\"\n",
        "    Safe version for Seq2Seq models (PEGASUS).\n",
        "    Ensures one output per article.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    strategy = strategy or {}\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(0, len(articles), batch_size):\n",
        "        batch = articles[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=1024).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                min_length=min_length,\n",
        "                **strategy\n",
        "            )\n",
        "\n",
        "        texts = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "        # Handle num_return_sequences > 1\n",
        "        num_return = strategy.get('num_return_sequences', 1)\n",
        "        if num_return > 1:\n",
        "            texts = [texts[j * num_return] for j in range(len(batch))]\n",
        "\n",
        "        # Ensure outputs match batch length\n",
        "        assert len(texts) == len(batch), f\"Mismatch in decoded outputs: {len(texts)} vs batch {len(batch)}\"\n",
        "\n",
        "        outputs.extend([t.strip() for t in texts])\n",
        "\n",
        "    # Ensure total outputs match total articles\n",
        "    assert len(outputs) == len(articles), f\"Total outputs {len(outputs)} != total articles {len(articles)}\"\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "EX92zRpq_kDx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:42:34.385531Z",
          "iopub.execute_input": "2025-09-27T22:42:34.385808Z",
          "iopub.status.idle": "2025-09-27T22:42:34.409262Z",
          "shell.execute_reply.started": "2025-09-27T22:42:34.385788Z",
          "shell.execute_reply": "2025-09-27T22:42:34.408620Z"
        }
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 6. Evaluation helpers ----------\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "def compute_rouge(preds, refs):\n",
        "  # preds and refs: lists of strings\n",
        "  # rouge.compute handles batch\n",
        "  result = rouge.compute(predictions=preds, references=refs)\n",
        "  # extract f-measure mid (if available)\n",
        "  def _get(k):\n",
        "    v = result.get(k)\n",
        "    if hasattr(v, 'mid'):\n",
        "      return v.mid.fmeasure\n",
        "    return v\n",
        "  return {'rouge1': _get('rouge1'), 'rouge2': _get('rouge2'), 'rougeL': _get('rougeL')}"
      ],
      "metadata": {
        "id": "nyPDyle5_qB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b14a6e-53a4-4a39-9b45-af1e0052792f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:42:34.410269Z",
          "iopub.execute_input": "2025-09-27T22:42:34.410561Z",
          "iopub.status.idle": "2025-09-27T22:42:35.855131Z",
          "shell.execute_reply.started": "2025-09-27T22:42:34.410536Z",
          "shell.execute_reply": "2025-09-27T22:42:35.854532Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments(\n",
        "    max_examples=MAX_EXAMPLES,\n",
        "    min_length=MIN_LENGTH,\n",
        "    save_outputs_path='results.jsonl',\n",
        "    mode='smol'  # ÂèØÈÄâ: 'smol' Êàñ 'pegasus'\n",
        "):\n",
        "    \"\"\"\n",
        "    ËøêË°åÊëòË¶ÅÂÆûÈ™åÔºàÊîØÊåÅ‰∏§ÁßçÊ®°Âºè: SmolLM Êàñ PEGASUSÔºâ„ÄÇ\n",
        "    ‰∏ç‰ΩøÁî®Êñ≠ÁÇπÁª≠Ë∑ë„ÄÇ\n",
        "    \"\"\"\n",
        "    # Âä†ËΩΩÊï∞ÊçÆ\n",
        "    ds = load_cnn_dailymail('test', max_examples=max_examples)\n",
        "    articles = [ex['article'] for ex in ds]\n",
        "    refs = [ex['highlights'] for ex in ds]\n",
        "\n",
        "    experiments = []\n",
        "\n",
        "    if mode == 'smol':\n",
        "        print('Loading SmolLM...')\n",
        "        tok_smol, model_smol = load_smol()\n",
        "        all_experiments = [\n",
        "            (\"E1: Instruction+Greedy\", lambda: [prompt_instruction(a, tok_smol) for a in articles], {'num_beams': 1, 'do_sample': False, 'no_repeat_ngram_size': 3}),\n",
        "            (\"E2: Instruction+Beam\", lambda: [prompt_instruction(a, tok_smol) for a in articles], {'num_beams': 2, 'do_sample': False, 'early_stopping': True, 'no_repeat_ngram_size': 3}),\n",
        "            (\"E3: FewShot+Beam\", lambda: [prompt_few_shot(a, tok_smol) for a in articles], {'num_beams': 2, 'do_sample': False, 'early_stopping': True, 'no_repeat_ngram_size': 3}),\n",
        "            (\"E4: FewShot+TopP\", lambda: [prompt_few_shot(a, tok_smol) for a in articles], {'do_sample': True, 'num_beams': 1, 'top_p': 0.95, 'temperature': 0.7, 'no_repeat_ngram_size': 3}),\n",
        "        ]\n",
        "\n",
        "        for name, prompt_fn, strategy in all_experiments:\n",
        "            print(f'üöÄ Running {name}...')\n",
        "            prompts = prompt_fn()\n",
        "            preds = generate_smol(tok_smol, model_smol, prompts, strategy, max_new_tokens=MAX_TOKENS, min_length=MIN_LENGTH)\n",
        "            scores = compute_rouge(preds, refs)\n",
        "            experiments.append({'name': name, 'scores': scores, 'preds': preds})\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    elif mode == 'pegasus':\n",
        "        print('Loading PEGASUS...')\n",
        "        tok_peg, model_peg = load_pegasus()\n",
        "        strategy_peg = {'num_beams': 8, 'length_penalty': 0.8}\n",
        "        preds_peg = generate_pegasus(tok_peg, model_peg, articles, max_new_tokens=MAX_TOKENS, min_length=MIN_LENGTH, strategy=strategy_peg)\n",
        "        scores_peg = compute_rouge(preds_peg, refs)\n",
        "        experiments.append({'name': 'PEGASUS', 'scores': scores_peg, 'preds': preds_peg})\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'smol' or 'pegasus'\")\n",
        "\n",
        "    # Ê±áÊÄªÁªìÊûú\n",
        "    rows = [{'method': exp['name'], **exp['scores']} for exp in experiments]\n",
        "    df = pd.DataFrame(rows).set_index('method')\n",
        "\n",
        "    # ‰øùÂ≠ò per-example JSONL\n",
        "    with open(save_outputs_path, 'w', encoding='utf-8') as fh:\n",
        "        for i in range(len(articles)):\n",
        "            rec = {'id': ds[i]['id'], 'article': articles[i], 'reference': refs[i]}\n",
        "            for exp in experiments:\n",
        "                rec[exp['name']] = exp['preds'][i]\n",
        "            fh.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    return experiments, df\n"
      ],
      "metadata": {
        "id": "pxrvUZOV_xFO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:42:35.855898Z",
          "iopub.execute_input": "2025-09-27T22:42:35.856172Z",
          "iopub.status.idle": "2025-09-27T22:42:35.869521Z",
          "shell.execute_reply.started": "2025-09-27T22:42:35.856146Z",
          "shell.execute_reply": "2025-09-27T22:42:35.868779Z"
        }
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 8. If run as script ----------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --------- Step 1: ËøêË°å SmolLM ----------\n",
        "    torch.cuda.empty_cache()\n",
        "    res_smol, df_smol = run_experiments(mode='smol', max_examples=MAX_EXAMPLES, min_length=MIN_LENGTH, save_outputs_path='smol_results.jsonl')\n",
        "    print(df_smol)\n",
        "    df_smol.to_csv('smol_rouge_scores.csv')\n",
        "\n",
        "    # --------- Step 2: ÈáäÊîæ GPUÔºåÂÜçËøêË°å PEGASUS ----------\n",
        "    torch.cuda.empty_cache()\n",
        "    res_peg, df_peg = run_experiments(mode='pegasus', max_examples=MAX_EXAMPLES, min_length=MIN_LENGTH, save_outputs_path='pegasus_results.jsonl')\n",
        "    print(df_peg)\n",
        "    df_peg.to_csv('pegasus_rouge_scores.csv')\n",
        "\n",
        "    # --------- Step 3: ÊãºÊé•ÁªìÊûú ----------\n",
        "    #df_smol = pd.read_csv('smol_rouge_scores.csv')\n",
        "    df_total = pd.concat([df_smol, df_peg], ignore_index=False)\n",
        "    print(df_total)\n",
        "    df_total.to_csv('summary_rouge_scores.csv')\n",
        "\n",
        "    print('\\n‚úÖ Â∑≤‰øùÂ≠òÊâÄÊúâËæìÂá∫Êñá‰ª∂')\n",
        "\n",
        "# End of notebook"
      ],
      "metadata": {
        "id": "xqrwep0dAEAQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553,
          "referenced_widgets": [
            "0fed14e79ddf4a2a927bb50a18344cf2",
            "3f4971b8eb7b4764a6b5dd0e9a971071",
            "7ec1e86d6e1e4b7ea4a33b4b99909039",
            "1ced22a73ebb4aaebd361953fe0564e3",
            "36be749f48f140fabd43960df22a003e",
            "b59f402e14d14c9882cfed694259230f",
            "b3d3087e86874e15ad72d83a8a38402c",
            "6335b3f8fa4145d681a86ce3afe01757",
            "0deb315e620f4858815c5495a31896de",
            "e4d31902ee1649aba2ec7c535aa66762",
            "5917bd3e2449443583eeae125634c857"
          ]
        },
        "outputId": "26abeb8b-e149-4b09-f276-e3a744d46ce7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-27T22:42:35.871857Z",
          "iopub.execute_input": "2025-09-27T22:42:35.872171Z",
          "iopub.status.idle": "2025-09-28T01:26:18.407520Z",
          "shell.execute_reply.started": "2025-09-27T22:42:35.872137Z",
          "shell.execute_reply": "2025-09-28T01:26:18.406734Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SmolLM...\n",
            "üöÄ Running E1: Instruction+Greedy...\n",
            "üöÄ Running E2: Instruction+Beam...\n",
            "üöÄ Running E3: FewShot+Beam...\n",
            "üöÄ Running E4: FewShot+TopP...\n",
            "                          rouge1    rouge2    rougeL\n",
            "method                                              \n",
            "E1: Instruction+Greedy  0.136200  0.016615  0.103861\n",
            "E2: Instruction+Beam    0.137873  0.015332  0.103841\n",
            "E3: FewShot+Beam        0.147683  0.017549  0.107364\n",
            "E4: FewShot+TopP        0.148044  0.015964  0.106671\n",
            "Loading PEGASUS...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fed14e79ddf4a2a927bb50a18344cf2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           rouge1  rouge2    rougeL\n",
            "method                             \n",
            "PEGASUS  0.321738  0.1335  0.237964\n",
            "                          rouge1    rouge2    rougeL\n",
            "method                                              \n",
            "E1: Instruction+Greedy  0.136200  0.016615  0.103861\n",
            "E2: Instruction+Beam    0.137873  0.015332  0.103841\n",
            "E3: FewShot+Beam        0.147683  0.017549  0.107364\n",
            "E4: FewShot+TopP        0.148044  0.015964  0.106671\n",
            "PEGASUS                 0.321738  0.133500  0.237964\n",
            "\n",
            "‚úÖ Â∑≤‰øùÂ≠òÊâÄÊúâËæìÂá∫Êñá‰ª∂\n"
          ]
        }
      ],
      "execution_count": 10
    }
  ]
}